{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9100808,"sourceType":"datasetVersion","datasetId":5492272},{"sourceId":9225461,"sourceType":"datasetVersion","datasetId":5579515},{"sourceId":12746226,"sourceType":"datasetVersion","datasetId":8057550}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom tensorflow.keras.applications import MobileNetV2, DenseNet121\nfrom tensorflow.keras.layers import (\n    Input, Dense, Concatenate, GlobalAveragePooling2D, Conv2D,\n    BatchNormalization, Activation, MultiHeadAttention, LayerNormalization, Reshape\n)\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras import regularizers\n\n# Parameters\nimg_height, img_width = 224, 224\nbatch_size = 32\nepochs = 100\nseed = 123  # For consistent split\n\n# Path (your full dataset directory with all classes)\ndata_dir = '/kaggle/input/S2RMCMD/sugarcane-disease_9026img/train'\n\n# 80% Training Dataset\ntrain_ds = tf.keras.preprocessing.image_dataset_from_directory(\n    data_dir,\n    validation_split=0.2,\n    subset='training',\n    seed=seed,\n    label_mode='int',\n    image_size=(img_height, img_width),\n    batch_size=batch_size,\n    \n)\n\n# 20% Testing Dataset\nval_ds = tf.keras.preprocessing.image_dataset_from_directory(\n    data_dir,\n    validation_split=0.2,\n    subset='validation',\n    seed=seed,\n    label_mode='int',\n    image_size=(img_height, img_width),\n    batch_size=batch_size,\n\n)\n\n# Class names\nclass_names = train_ds.class_names\n\n# Prefetch for performance\nAUTOTUNE = tf.data.AUTOTUNE\ntrain_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\nval_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n\n# Build model\ninput_layer = Input(shape=(img_height, img_width, 3))\n\nmobilenet_base = MobileNetV2(include_top=False, weights='imagenet', input_tensor=input_layer)\ndensenet_base = DenseNet121(include_top=False, weights='imagenet', input_tensor=input_layer)\n\nmobilenet_base.trainable = True\ndensenet_base.trainable = True\n\n# Global Average Pooling branch\ngap1 = GlobalAveragePooling2D()(mobilenet_base.output)\ngap2 = GlobalAveragePooling2D()(densenet_base.output)\ngap_concat = Concatenate()([gap1, gap2])\n\n# Conv + Self-Attention branch - MobileNetV2\nx1 = mobilenet_base.output\nfor filters in [32, 64, 128, 256]:\n    x1 = Conv2D(filters, (1, 1), padding='same')(x1)\n    x1 = BatchNormalization()(x1)\n    x1 = Activation('relu')(x1)\n\nx1_reshape = Reshape((-1, x1.shape[-1]))(x1)\nx1_attn = MultiHeadAttention(num_heads=4, key_dim=32)(x1_reshape, x1_reshape)\nx1_attn = LayerNormalization()(x1_attn + x1_reshape)\nx1 = Reshape((x1.shape[1], x1.shape[2], x1.shape[3]))(x1_attn)\nx1 = GlobalAveragePooling2D()(x1)\n\n# Conv + Self-Attention branch - DenseNet201\nx2 = densenet_base.output\nfor filters in [32, 64, 128, 256]:\n    x2 = Conv2D(filters, (1, 1), padding='same')(x2)\n    x2 = BatchNormalization()(x2)\n    x2 = Activation('relu')(x2)\n\nx2_reshape = Reshape((-1, x2.shape[-1]))(x2)\nx2_attn = MultiHeadAttention(num_heads=4, key_dim=32)(x2_reshape, x2_reshape)\nx2_attn = LayerNormalization()(x2_attn + x2_reshape)\nx2 = Reshape((x2.shape[1], x2.shape[2], x2.shape[3]))(x2_attn)\nx2 = GlobalAveragePooling2D()(x2)\n\n# Final concatenation\nconv_concat = Concatenate()([x1, x2])\nfinal_concat = Concatenate()([gap_concat, conv_concat])\n\n# Output\noutput = Dense(len(class_names), activation='softmax', kernel_regularizer=regularizers.l2(0.01))(final_concat)\nmodel = Model(inputs=input_layer, outputs=output)\nmodel.summary()\n# Compile\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n# Train\nhistory = model.fit(train_ds, validation_data=val_ds, epochs=epochs)\n\n# Save model\nmodel.save('mobilenet_densenet_attention_80_20_split.h5')\n\n# Evaluate on validation set\ny_true, y_pred = [], []\nfor images, labels_batch in val_ds:\n    preds = model.predict(images)\n    y_pred.extend(np.argmax(preds, axis=1))\n    y_true.extend(labels_batch.numpy())\n\n# Confusion Matrix and Classification Report\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(y_true, y_pred))\n\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_true, y_pred, target_names=class_names))\n\n# Plot Accuracy and Loss\nplt.figure(figsize=(12, 4))\n\nplt.subplot(1, 2, 1)\nplt.plot(history.history['accuracy'], label='Train Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.title('Model Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(history.history['loss'], label='Train Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.title('Model Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}